## MTLNet: Multi-task Learning with Limited Data for Source-free Cross-domain Few-shot Semantic Segmentation
Introduction
Abstract—Cross-domain few-shot semantic segmentation aims to achieve efficient image segmentation within a target domain using limited annotated data. Existing methods often require access to source data during training. However, with growing concerns over data privacy and the need to reduce data transfer and training costs, developing a CD-FSS solution that does not require access to source data is necessary. To address this issue, this paper proposes a multi-task learning-based Source-free cross-domain few-shot semantic segmentation method, which optimizes and fine-tunes the model using a minimal amount of target domain data without accessing source domain data, thereby improving segmentation performance. Firstly, we propose a Hierarchical Mask module to prevent model overfitting on limited data. This module combines contrastive learning and supervised learning; contrastive learning enhances the model’s generalization ability by learning universal feature representations from unlabeled data, while supervised learning uses a small amount of labeled data for precise model training, ensuring stable performance in the target domain. Secondly, we introduce a multi-task loss function that helps the model capture general patterns and structures within the data and reduces overfitting to a single task, thereby improving the model’s robustness when handling new data. Finally, to optimize the synergy between multiple tasks, we propose a Triplet Contextual Alignment strategy, which enhances the model’s performance in semantic segmentation tasks by properly managing the dependencies between different tasks. Experimental results show that our model significantly outperforms the CD-FSS baseline, demonstrating the new potential of multitask learning when trained on extremely small amounts of target domain data. The code and dataset are available on GitHub-Link.
